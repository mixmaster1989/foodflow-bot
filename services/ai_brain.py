
import json
import logging
import aiohttp
import asyncio
import time
from typing import List, Dict
from config import settings
from monitoring import stats, get_ai_semaphore

logger = logging.getLogger("ai.brain")

class AIBrainService:
    """Core AI Brain Service for semantic understanding of user input."""

    MODEL = "google/gemini-2.5-flash-lite" # Fast, Smart enough

    SYSTEM_PROMPT = """
–¢—ã ‚Äî –ú–æ–∑–≥ –±–æ—Ç–∞ FoodFlow. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–Ω—è—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ.
–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è (intents):
1. 'log_consumption' ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ø–í–ù–û —É–∫–∞–∑–∞–ª, —á—Ç–æ –°–™–ï–õ ("–°—ä–µ–ª —è–±–ª–æ–∫–æ", "–ù–∞ –∑–∞–≤—Ç—Ä–∞–∫ –∫–æ—Ñ–µ", "–í—ã–ø–∏–ª —á–∞–π", "–£–∂–∏–Ω: –º–∞–∫–∞—Ä–æ–Ω—ã").
2. 'add_to_fridge' ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ø–í–ù–û —É–∫–∞–∑–∞–ª, —á—Ç–æ –ö–£–ü–ò–õ –∏–ª–∏ —Ö–æ—á–µ—Ç –î–û–ë–ê–í–ò–¢–¨ ("–ö—É–ø–∏–ª –º–æ–ª–æ–∫–∞", "–í —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ —Å—ã—Ä", "+ –±–∞–Ω–∞–Ω").
3. 'unknown' ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –≥–ª–∞–≥–æ–ª–∞ –¥–µ–π—Å—Ç–≤–∏—è –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ø—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞: "–ë–∞–Ω–∞–Ω", "–ú–æ–ª–æ–∫–æ").

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤–µ—Ä–Ω—É—Ç—å JSON –æ–±—ä–µ–∫—Ç:
{
  "intent": "log_consumption" | "add_to_fridge" | "unknown",
  "product": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞" (–∏–ª–∏ null),
  "weight": 100 (—á–∏—Å–ª–æ –≤ –≥—Ä–∞–º–º–∞—Ö, –∏–ª–∏ null –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ),
  "quantity": 1 (—á–∏—Å–ª–æ —à—Ç—É–∫, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ, –∏–Ω–∞—á–µ 1),
  "original_text": "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç"
}

–ü—Ä–∞–≤–∏–ª–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏:
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç "–¥–≤–∞ —è–±–ª–æ–∫–∞", quantity=2, weight=null (–µ—Å–ª–∏ –≤–µ—Å –Ω–µ –Ω–∞–∑–≤–∞–Ω —è–≤–Ω–æ).
- –ï—Å–ª–∏ "—è–±–ª–æ–∫–æ 200–≥", product="—è–±–ª–æ–∫–æ", weight=200.
- –ï—Å–ª–∏ "–ø–æ–ª–∫–∏–ª–æ", weight=500.
- –°–¢–†–û–ì–û: –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ "–Ø–±–ª–æ–∫–æ" (–±–µ–∑ "—Å—ä–µ–ª", "–∫—É–ø–∏–ª") -> intent="unknown".
- –°–¢–†–û–ì–û: –ï—Å–ª–∏ –ì–µ—Ä–±–∞–ª–∞–π—Ñ ("–ö–æ–∫—Ç–µ–π–ª—å –§1", "–ê–ª–æ—ç") –±–µ–∑ –≥–ª–∞–≥–æ–ª–∞ -> intent="unknown".

–ù–ï –ü–ò–®–ò –ù–ò–ß–ï–ì–û –ö–†–û–ú–ï JSON.
"""

    @classmethod
    async def analyze_text(cls, text: str) -> dict | None:
        """Call LLM to analyze text and return structured data."""
        
        headers = {
            "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://foodflow.app",
            "X-Title": "FoodFlow Bot",
        }

        payload = {
            "model": cls.MODEL,
            "messages": [
                {"role": "system", "content": cls.SYSTEM_PROMPT},
                {"role": "user", "content": text}
            ],
            "response_format": {"type": "json_object"}
        }


        # Use semaphore to limit concurrent AI calls (Phase 1 optimization)
        semaphore = get_ai_semaphore(max_concurrent=5)
        
        async with semaphore:
            start_time = time.time()
            for attempt in range(2):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            "https://openrouter.ai/api/v1/chat/completions",
                            headers=headers,
                            json=payload,
                            timeout=10
                        ) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                # Clean output just in case
                                content = content.replace("```json", "").replace("```", "").strip()
                                
                                # Track stats
                                duration_ms = (time.time() - start_time) * 1000
                                stats.record_ai_call(duration_ms)
                                
                                return json.loads(content)
                            else:
                                logger.warning(f"AI Brain error {response.status}: {await response.text()}")
                                stats.record_error()
                except Exception as e:
                    logger.error(f"AI Brain exception: {e}")
                    stats.record_error()
                    
                await asyncio.sleep(0.5)
            
        return None

    @classmethod
    async def resolve_herbalife_product(cls, text: str, products_context: List[Dict]) -> str | None:
        """Use AI to match user input to a specific Herbalife Product ID."""
        
        # Prepare a compact list of products for the prompt
        compact_list = [
            {"id": p["id"], "name": p["name"], "aliases": p.get("aliases", [])}
            for p in products_context
        ]

        prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–¥—É–∫—Ü–∏–∏ Herbalife. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º ID –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.
–ï—Å–ª–∏ –≤–æ –≤–≤–æ–¥–µ —É–∫–∞–∑–∞–Ω –≤–∫—É—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–¥—ã–Ω—è', '–º–∞–Ω–≥–æ', '—à–æ–∫–æ–ª–∞–¥'), –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–±–µ—Ä–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π ID.

–°–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:
{json.dumps(compact_list, ensure_ascii=False, indent=2)}

–í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{text}"

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤–µ—Ä–Ω—É—Ç—å JSON:
{{
  "matched_product_id": "id_from_list" (–∏–ª–∏ null –µ—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è),
  "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω —ç—Ç–æ—Ç ID"
}}
"""

        headers = {
            "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://foodflow.app",
            "X-Title": "FoodFlow Bot",
        }

        payload = {
            "model": cls.MODEL,
            "messages": [
                {"role": "system", "content": "Return ONLY JSON."},
                {"role": "user", "content": prompt}
            ],
            "response_format": {"type": "json_object"}
        }


        # Use semaphore to limit concurrent AI calls
        semaphore = get_ai_semaphore(max_concurrent=5)
        
        async with semaphore:
            start_time = time.time()
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://openrouter.ai/api/v1/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=10
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            content = data['choices'][0]['message']['content']
                            logger.info(f"Herbalife AI Raw Response: {content[:200]}")
                            result = json.loads(content)
                            matched_id = result.get("matched_product_id")
                            logger.info(f"Herbalife AI Matched ID: {matched_id}, Reason: {result.get('reason', 'N/A')}")
                            
                            # Track stats
                            duration_ms = (time.time() - start_time) * 1000
                            stats.record_ai_call(duration_ms)
                            
                            return matched_id
                        else:
                            logger.warning(f"Herbalife AI HTTP Error: {response.status}")
                            stats.record_error()
            except Exception as e:
                logger.error(f"Herbalife Resolution AI Error: {e}")
                stats.record_error()
            

    @classmethod
    async def analyze_image(cls, message_or_path: any, prompt: str = "Describe this image.") -> str | None:
        """Analyze image using Vision model. Accepts aiogram Message or file path."""
        import base64
        import os
        
        # Determine image source
        b64_image = None
        
        try:
            if isinstance(message_or_path, str):
                # It's a file path
                with open(message_or_path, "rb") as image_file:
                    b64_image = base64.b64encode(image_file.read()).decode('utf-8')
            else:
                # Assume it's an aiogram Message object
                # We need to download it first. This requires the 'bot' instance.
                # Since we don't have easy access to bot instance here without circular imports,
                # we'll assume the caller handles downloading if it's complex.
                # BUT, wait! We can get bot from message.bot
                bot = message_or_path.bot
                if message_or_path.photo:
                    file_id = message_or_path.photo[-1].file_id
                    file = await bot.get_file(file_id)
                    file_path = file.file_path
                    
                    # Download to memory
                    io_obj = await bot.download_file(file_path)
                    b64_image = base64.b64encode(io_obj.read()).decode('utf-8')

            if not b64_image:
                logger.error("Could not obtain base64 image data")
                return None

            headers = {
                "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://foodflow.app",
                "X-Title": "FoodFlow Bot",
            }

            VISION_MODELS = [
                "google/gemini-2.5-flash-lite-preview-09-2025", # Main
                "qwen/qwen2.5-vl-72b-instruct:free"               # Fallback
            ]

            async with aiohttp.ClientSession() as session:
                for model in VISION_MODELS:
                    try:
                        payload = {
                            "model": model,
                            "messages": [
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "text", "text": prompt},
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/jpeg;base64,{b64_image}"
                                            }
                                        }
                                    ]
                                }
                            ]
                        }

                        logger.info(f"Vision Analysis: Trying model {model}...")
                        
                        async with session.post(
                            "https://openrouter.ai/api/v1/chat/completions",
                            headers=headers,
                            json=payload,
                            timeout=30 # Increased timeout for Vision
                        ) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                logger.info(f"Vision Analysis ({model}): {content[:100]}...")
                                return content
                            else:
                                error_text = await response.text()
                                logger.warning(f"Vision API Error ({model}): {response.status} - {error_text}")
                                # Continue to next model
                    except Exception as e:
                        logger.error(f"Vision Analysis Exception ({model}): {e}")
                        # Continue to next model
            
            logger.error("All Vision models failed.")
            return None

        except Exception as e:
            logger.error(f"Vision Analysis Outer Exception: {e}", exc_info=True)
            return None


    @classmethod
    async def summarize_fridge(cls, product_list: List[str]) -> Dict | None:
        """Generate a structured summary (text + tags) of fridge contents."""
        
        products_str = ", ".join(product_list[:40]) # Limit context window
        
        headers = {
            "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://foodflow.app",
            "X-Title": "FoodFlow Bot",
        }

        prompt = f"""
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –≤–µ–∂–ª–∏–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç FoodFlow. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–µ –∏ –≤–µ—Ä–Ω—É—Ç—å JSON.

–°–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {products_str}

–í–ï–†–ù–ò JSON –æ–±—ä–µ–∫—Ç:
{{
  "summary": "–¢–µ–∫—Å—Ç —Å–∞–º–º–∞—Ä–∏ (–º–∞–∫—Å 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è). –¢–æ–Ω: –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ª–µ–≥–∫–∞—è –∏—Ä–æ–Ω–∏—è. –ù–ò–ö–ê–ö–û–ì–û —Å–ª–µ–Ω–≥–∞.",
  "tags": [
    {{"tag": "–ú–æ–ª–æ–∫–æ", "emoji": "ü•õ"}}, 
    {{"tag": "–ö—É—Ä–∏—Ü–∞", "emoji": "üçó"}}
  ] 
}}

–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è tags:
- –í—ã–±–µ—Ä–∏ 3-4 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.
- –ö–†–ò–¢–ò–ß–ù–û: –¢–µ–≥–∏ ("tag") –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –°–õ–û–í–ê–ú–ò, –∫–æ—Ç–æ—Ä—ã–µ –§–ò–ó–ò–ß–ï–°–ö–ò –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
- "emoji": –ü–æ–¥–±–µ—Ä–∏ –û–î–ò–ù —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–æ —Å–º—ã—Å–ª—É. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–¥–∫–∏–µ —Å–∏–º–≤–æ–ª—ã.
- –ü—Ä–∏–º–µ—Ä: –µ—Å–ª–∏ –µ—Å—Ç—å "–ú–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ", tag="–ú–æ–ª–æ–∫–æ", emoji="ü•õ". 
- –ó–ê–ü–†–ï–©–ï–ù–û: –ü—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ.
- –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–Ω—ã–π, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤.

–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–û–õ–¨–ö–û JSON.
"""

        payload = {
            "model": cls.MODEL,
            "messages": [
                {"role": "system", "content": "You are a helpful culinary assistant. Return ONLY JSON."},
                {"role": "user", "content": prompt}
            ],
            "response_format": {"type": "json_object"}
        }


        # Use semaphore to limit concurrent AI calls
        semaphore = get_ai_semaphore(max_concurrent=5)
        
        async with semaphore:
            start_time = time.time()
            for attempt in range(2):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            "https://openrouter.ai/api/v1/chat/completions",
                            headers=headers,
                            json=payload,
                            timeout=15
                        ) as response:
                            if response.status == 200:
                                data = await response.json()
                                content = data['choices'][0]['message']['content']
                                # Parse JSON
                                try:
                                    result = json.loads(content)
                                    if "summary" in result:
                                        # Track stats
                                        duration_ms = (time.time() - start_time) * 1000
                                        stats.record_ai_call(duration_ms)
                                        return result
                                except json.JSONDecodeError:
                                    logger.warning(f"AI Summary JSON Error: {content}")
                                    stats.record_error()
                            else:
                                logger.warning(f"AI Summary error {response.status}")
                                stats.record_error()
                except Exception as e:
                    logger.error(f"AI Summary exception: {e}")
                    stats.record_error()
                    
                await asyncio.sleep(0.5)
            
        return None
